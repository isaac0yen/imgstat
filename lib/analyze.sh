#!/usr/bin/env bash

source "$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)/utils.sh"

cmd_analyze() {
  local dir="$1"
  echo "Analyzing codebase for remote image references in $dir..."

  # â”€â”€ Step 1: Extract all http/https URLs from common code files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  local all_urls=()
  mapfile -t all_urls < <(find "$dir" \
    -type d \( -name "node_modules" -o -name ".git" -o -name "dist" -o -name "build" -o -name "vendor" -o -name ".next" -o -name "coverage" \) -prune -o \
    -type f \( -name "*.html" -o -name "*.jsx" -o -name "*.tsx" -o -name "*.js" -o -name "*.ts" \
               -o -name "*.vue" -o -name "*.css" -o -name "*.scss" -o -name "*.php" \
               -o -name "*.py" -o -name "*.rb" -o -name "*.svelte" -o -name "*.astro" \) \
    -exec grep -hoE "https?://[^\"')[:space:]]+" {} + 2>/dev/null | sort -u)

  if [[ ${#all_urls[@]} -eq 0 ]]; then
    echo "No remote URLs found in code files."
    return 0
  fi

  echo "Found ${#all_urls[@]} unique URL(s). Classifying..."
  echo ""

  # â”€â”€ Step 2: Classify each URL via 3-tier pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  local image_extensions=("jpg" "jpeg" "png" "gif" "webp" "avif" "svg" "bmp" "tiff" "tif" "ico")

  local queued_urls=()

  for url in "${all_urls[@]}"; do
    # Strip query/fragment to inspect the path cleanly
    local path
    path=$(echo "$url" | sed 's/[?#].*//')
    local lower_path
    lower_path=$(echo "$path" | tr '[:upper:]' '[:lower:]')

    # â”€â”€ Tier 1: Obvious non-image check (fast, no network) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if is_obvious_non_image "$url"; then
      printf "  \033[2mâ­  Skip  (pattern match) : %s\033[0m\n" "$url"
      continue
    fi

    # â”€â”€ Tier 2: Image extension in path (fast, no network) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    local ext
    ext=$(echo "${lower_path##*.}" | sed 's/[^a-z]//g')
    local is_image_ext=false
    for img_ext in "${image_extensions[@]}"; do
      if [[ "$ext" == "$img_ext" ]]; then
        is_image_ext=true
        break
      fi
    done

    if [[ "$is_image_ext" == "true" ]]; then
      printf "  \033[1;32mâœ“  Queue (extension)     : %s\033[0m\n" "$url"
      queued_urls+=("$url")
      continue
    fi

    # â”€â”€ Tier 3: HTTP HEAD Content-Type check (network, no body download) â”€â”€â”€â”€â”€
    printf "  \033[33m?  Check (HEAD request)  : %s\033[0m" "$url"
    if check_content_type_is_image "$url"; then
      printf "\r  \033[1;32mâœ“  Queue (content-type)  : %s\033[0m\n" "$url"
      queued_urls+=("$url")
    else
      printf "\r  \033[2mâ­  Skip  (not image CT)  : %s\033[0m\n" "$url"
    fi
  done

  echo ""

  if [[ ${#queued_urls[@]} -eq 0 ]]; then
    echo "No image URLs found after classification."
    return 0
  fi

  echo "Fetching dimensions for ${#queued_urls[@]} image URL(s)..."
  echo ""

  # â”€â”€ Step 3: Download & measure queued image URLs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  local TEMP_DIR
  TEMP_DIR=$(mktemp -d)
  trap 'rm -rf "$TEMP_DIR"' EXIT

  # Output file setup
  local rules_dir="$dir/.agent/rules"
  local rules_file="$rules_dir/image_dimensions.md"
  mkdir -p "$rules_dir"

  # Write header
  cat << 'EOF' > "$rules_file"
# Codebase Remote Images

> [!NOTE]
> This file is strictly auto-generated by `imgstat`.
> It maps remote image URLs found in the codebase to their exact physical dimensions.
> AI assistants should use this dictionary when asked about layout constraints, native sizes, or aspect ratios of referenced images.

| Documented URL | Detected Size (W x H) |
|---|---|
EOF

  local processed=0

  for url in "${queued_urls[@]}"; do
    # Download the image content only
    wget -q --content-disposition --max-redirect=5 \
      --user-agent="Mozilla/5.0" \
      -P "$TEMP_DIR" "$url" 2>/dev/null || true

    local all_files=( "$TEMP_DIR"/* )
    local found_image=""

    for file in "${all_files[@]}"; do
      [[ ! -f "$file" ]] && continue
      local mimetype
      mimetype=$(file -b --mime-type "$file" 2>/dev/null || true)
      if [[ "$mimetype" == image/* ]]; then
        found_image="$file"
        break
      fi
    done

    if [[ -n "$found_image" ]]; then
      local dim
      dim=$(get_dimensions "$found_image" || true)
      if [[ -n "$dim" ]]; then
        local w="${dim% *}"
        local h="${dim#* }"
        echo "| \`$url\` | **${w}x${h}** |" >> "$rules_file"
        printf "  \033[1;32mğŸ“ %s â†’ %sx%s\033[0m\n" "$url" "$w" "$h"
        processed=$((processed + 1))
      fi
    fi

    # Clean temp dir for next URL to avoid collisions
    rm -rf "${TEMP_DIR:?}"/*
  done

  echo ""
  echo "Analysis complete!"
  echo "Documented $processed valid image reference(s) into: $rules_file"
  echo "This file can now be read by autonomous coding agents."
}
